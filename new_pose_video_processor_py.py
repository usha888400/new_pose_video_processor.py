# -*- coding: utf-8 -*-
"""new_pose_video_processor.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-wUQnJDNgSiVdn7yL4hbKr6xgE864m9
"""



from google.colab import files
import os

# Upload video file
print("Please upload your video file:")
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]
print(f"Uploaded file: {uploaded_filename}")

# Check if file exists and get info
if os.path.exists(uploaded_filename):
    file_size = os.path.getsize(uploaded_filename)
    print(f"File size: {file_size} bytes")
else:
    print("Error: File not found after upload!")

!pip install -q ultralytics

from ultralytics import YOLO
import cv2
import numpy as np

# Load the largest/most accurate pose model for better detection
print("Loading YOLO model...")
model = YOLO("yolov8x-pose.pt")

# Configure model for maximum detection
model.overrides['conf'] = 0.1  # Very low confidence threshold
model.overrides['iou'] = 0.3   # Lower IoU to detect overlapping people
model.overrides['max_det'] = 50  # Allow up to 50 detections per frame

# Use the uploaded filename
input_path = uploaded_filename
output_path = 'annotated_output_enhanced.mp4'

print(f"Input file: {input_path}")
print(f"Output file: {output_path}")

# Test if video can be opened
print("Testing video file...")
test_cap = cv2.VideoCapture(input_path)
if not test_cap.isOpened():
    print("❌ Error: Cannot open video file!")
    print("Common issues:")
    print("1. File might be corrupted")
    print("2. Unsupported video format")
    print("3. File not uploaded properly")

    # Try different video backends
    print("\nTrying different backends...")
    backends = [cv2.CAP_FFMPEG, cv2.CAP_GSTREAMER, cv2.CAP_V4L2]
    for i, backend in enumerate(backends):
        test_cap = cv2.VideoCapture(input_path, backend)
        if test_cap.isOpened():
            print(f"✅ Success with backend {i}")
            break
        test_cap.release()

    if not test_cap.isOpened():
        print("❌ Could not open video with any backend")
        exit()

# Get video properties
width = int(test_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(test_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = test_cap.get(cv2.CAP_PROP_FPS)
total_frames = int(test_cap.get(cv2.CAP_PROP_FRAME_COUNT))

print(f"Video dimensions: {width}x{height}")
print(f"Original FPS: {fps}")
print(f"Total frames: {total_frames}")

# Validate video properties
if width == 0 or height == 0 or fps == 0:
    print("❌ Invalid video properties detected!")
    print("Trying to extract a frame to check video integrity...")

    ret, frame = test_cap.read()
    if ret:
        print("✅ Frame extracted successfully")
        height, width = frame.shape[:2]
        print(f"Frame dimensions from actual frame: {width}x{height}")
        fps = 25.0  # Default FPS if detection fails
        print(f"Using default FPS: {fps}")
    else:
        print("❌ Cannot read frames from video")
        exit()

test_cap.release()

# Now process the video
cap = cv2.VideoCapture(input_path)

# Video writer with better codec options
fourcc_options = [
    cv2.VideoWriter_fourcc(*'mp4v'),
    cv2.VideoWriter_fourcc(*'XVID'),
    cv2.VideoWriter_fourcc(*'MJPG'),
    cv2.VideoWriter_fourcc(*'X264')
]

out = None
for fourcc in fourcc_options:
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    if out.isOpened():
        print(f"✅ Video writer opened successfully")
        break
    out.release()

if out is None or not out.isOpened():
    print("❌ Could not create video writer")
    exit()

frame_id = 0
processed_frames = 0

print("Starting video processing...")

while cap.isOpened():
    success, frame = cap.read()
    if not success:
        print(f"End of video or read error at frame {frame_id}")
        break

    try:
        # Apply more aggressive enhancement for difficult cases
        enhanced_frame = frame.copy()

        # Method 1: CLAHE enhancement
        lab = cv2.cvtColor(enhanced_frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))  # More aggressive
        enhanced_l = clahe.apply(l)
        enhanced_lab = cv2.merge([enhanced_l, a, b])
        enhanced_frame = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)

        # Method 2: Gamma correction for better visibility
        gamma = 1.2
        enhanced_frame = np.power(enhanced_frame / 255.0, gamma) * 255.0
        enhanced_frame = enhanced_frame.astype(np.uint8)

        # Method 3: Unsharp masking for edge enhancement
        gaussian = cv2.GaussianBlur(enhanced_frame, (0, 0), 2.0)
        enhanced_frame = cv2.addWeighted(enhanced_frame, 1.5, gaussian, -0.5, 0)

        # Method 4: Slight denoising
        enhanced_frame = cv2.bilateralFilter(enhanced_frame, 5, 50, 50)

        # Multiple detection passes for maximum coverage
        # Pass 1: Standard detection
        results1 = model(
            enhanced_frame,
            verbose=False,
            conf=0.05,  # Very low confidence to catch distant/unclear people
            iou=0.2,    # Low IoU to detect overlapping people
            imgsz=640,  # Standard size
            agnostic_nms=True,  # More aggressive detection
            max_det=50  # Allow many detections
        )

        # Pass 2: Higher resolution for small people
        results2 = model(
            enhanced_frame,
            verbose=False,
            conf=0.1,   # Slightly higher confidence
            iou=0.3,
            imgsz=1280, # Higher resolution for small/distant people
            agnostic_nms=True,
            max_det=50
        )

        # Use the result with more detections
        results = results1 if len(results1[0]) >= len(results2[0]) else results2

        # Enhanced annotation with person counting
        annotated_frame = enhanced_frame.copy()
        people_count = 0

        if results[0].keypoints is not None and len(results[0].keypoints) > 0:
            keypoints = results[0].keypoints.xy.cpu().numpy()
            confidences = results[0].boxes.conf.cpu().numpy() if results[0].boxes is not None else []

            people_count = len(keypoints)

            # Draw each detected person
            for i, person_kpts in enumerate(keypoints):
                # Use different colors for different people
                colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
                color = colors[i % len(colors)]

                # Count valid keypoints for this person
                valid_kpts = sum(1 for kpt in person_kpts if kpt[0] > 0 and kpt[1] > 0)

                # Draw keypoints (even with few valid points)
                for kpt in person_kpts:
                    if kpt[0] > 0 and kpt[1] > 0:
                        cv2.circle(annotated_frame, (int(kpt[0]), int(kpt[1])), 4, color, -1)

                # Draw skeleton connections
                connections = [
                    (0, 1), (0, 2), (1, 3), (2, 4),  # Head
                    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Arms
                    (5, 11), (6, 12), (11, 12),  # Torso
                    (11, 13), (13, 15), (12, 14), (14, 16)  # Legs
                ]

                for connection in connections:
                    pt1_idx, pt2_idx = connection
                    if (pt1_idx < len(person_kpts) and pt2_idx < len(person_kpts) and
                        person_kpts[pt1_idx][0] > 0 and person_kpts[pt1_idx][1] > 0 and
                        person_kpts[pt2_idx][0] > 0 and person_kpts[pt2_idx][1] > 0):

                        pt1 = (int(person_kpts[pt1_idx][0]), int(person_kpts[pt1_idx][1]))
                        pt2 = (int(person_kpts[pt2_idx][0]), int(person_kpts[pt2_idx][1]))
                        cv2.line(annotated_frame, pt1, pt2, color, 2)

                # Add person ID number
                if len(person_kpts) > 0 and person_kpts[0][0] > 0:  # Use head position
                    cv2.putText(annotated_frame, f'P{i+1}',
                               (int(person_kpts[0][0]), int(person_kpts[0][1])-10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # Add people count to frame
        cv2.putText(annotated_frame, f'People Detected: {people_count}',
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write frame
        out.write(annotated_frame)
        processed_frames += 1

        # Progress indicator
        if frame_id % 30 == 0:
            if total_frames > 0:
                progress = (frame_id / total_frames) * 100
                print(f"Processing: {progress:.1f}% complete ({people_count} people in current frame)")
            else:
                print(f"Processed {frame_id} frames ({people_count} people in current frame)")

        frame_id += 1

    except Exception as e:
        print(f"Error processing frame {frame_id}: {str(e)}")
        continue

cap.release()
out.release()

print(f"✅ Processing complete!")
print(f"Total frames processed: {processed_frames}")
print(f"Enhanced annotated video saved as: {output_path}")

# Check if output file was created
if os.path.exists(output_path):
    output_size = os.path.getsize(output_path)
    print(f"Output file size: {output_size} bytes")

    if output_size > 0:
        print("Downloading result...")
        files.download(output_path)
    else:
        print("❌ Output file is empty!")
else:
    print("❌ Output file was not created!")

# List all files in current directory for debugging
print("\nFiles in current directory:")
for file in os.listdir('.'):
    if file.endswith(('.mp4', '.avi', '.mov')):
        size = os.path.getsize(file)
        print(f"  {file}: {size} bytes")


!wget -q https://raw.githubusercontent.com/USERNAME/REPO/main/pose_video_processor.py -O script.py && python script.py
